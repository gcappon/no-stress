{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b12c3be",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e81d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import shap \n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc5de7e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b86ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('data', 'feature_extracted', 'feature_extracted_dataset_1.npz'), allow_pickle = True)\n",
    "\n",
    "#training dataset \n",
    "x_train = data['x_train']\n",
    "y_train = data['y_train']\n",
    "idx_train = data['idx_train']\n",
    "\n",
    "#test dataset \n",
    "x_test = data['x_test']\n",
    "idx_test = data['idx_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c97fc1e",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "51cb74dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_82 (InputLayer)          [(None, 60, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " input_83 (InputLayer)          [(None, 60, 8)]      0           []                               \n",
      "                                                                                                  \n",
      " masking_56 (Masking)           (None, 60, 64)       0           ['input_82[0][0]']               \n",
      "                                                                                                  \n",
      " masking_57 (Masking)           (None, 60, 8)        0           ['input_83[0][0]']               \n",
      "                                                                                                  \n",
      " input_84 (InputLayer)          [(None, 60, 12)]     0           []                               \n",
      "                                                                                                  \n",
      " gru_91 (GRU)                   (None, 60, 32)       9408        ['masking_56[0][0]']             \n",
      "                                                                                                  \n",
      " gru_94 (GRU)                   (None, 60, 32)       4032        ['masking_57[0][0]']             \n",
      "                                                                                                  \n",
      " gru_97 (GRU)                   (None, 60, 32)       4416        ['input_84[0][0]']               \n",
      "                                                                                                  \n",
      " gru_92 (GRU)                   (None, 60, 16)       2400        ['gru_91[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_95 (GRU)                   (None, 60, 16)       2400        ['gru_94[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_98 (GRU)                   (None, 60, 16)       2400        ['gru_97[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_93 (GRU)                   (None, 8)            624         ['gru_92[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_96 (GRU)                   (None, 8)            624         ['gru_95[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_99 (GRU)                   (None, 8)            624         ['gru_98[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 24)           0           ['gru_93[0][0]',                 \n",
      "                                                                  'gru_96[0][0]',                 \n",
      "                                                                  'gru_99[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 16)           400         ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            17          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,345\n",
      "Trainable params: 27,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 22s 207ms/step - loss: 0.6900 - accuracy: 0.5350 - val_loss: 0.6878 - val_accuracy: 0.5024\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 3s 133ms/step - loss: 0.6789 - accuracy: 0.5634 - val_loss: 0.6776 - val_accuracy: 0.5966\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 3s 134ms/step - loss: 0.6683 - accuracy: 0.6165 - val_loss: 0.6639 - val_accuracy: 0.5942\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 4s 140ms/step - loss: 0.6573 - accuracy: 0.6304 - val_loss: 0.6489 - val_accuracy: 0.6256\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 4s 143ms/step - loss: 0.6475 - accuracy: 0.6341 - val_loss: 0.6345 - val_accuracy: 0.6498\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 4s 142ms/step - loss: 0.6394 - accuracy: 0.6437 - val_loss: 0.6324 - val_accuracy: 0.6522\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 4s 140ms/step - loss: 0.6388 - accuracy: 0.6316 - val_loss: 0.6306 - val_accuracy: 0.6570\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 4s 143ms/step - loss: 0.6344 - accuracy: 0.6401 - val_loss: 0.6276 - val_accuracy: 0.6401\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 4s 143ms/step - loss: 0.6358 - accuracy: 0.6335 - val_loss: 0.6279 - val_accuracy: 0.6522\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 4s 150ms/step - loss: 0.6324 - accuracy: 0.6479 - val_loss: 0.6237 - val_accuracy: 0.6570\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.6337 - accuracy: 0.6389 - val_loss: 0.6234 - val_accuracy: 0.6401\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.6323 - accuracy: 0.6353 - val_loss: 0.6272 - val_accuracy: 0.6522\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.6344 - accuracy: 0.6371 - val_loss: 0.6209 - val_accuracy: 0.6498\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 5s 211ms/step - loss: 0.6325 - accuracy: 0.6322 - val_loss: 0.6275 - val_accuracy: 0.6570\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 6s 227ms/step - loss: 0.6277 - accuracy: 0.6335 - val_loss: 0.6191 - val_accuracy: 0.6570\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 5s 186ms/step - loss: 0.6307 - accuracy: 0.6389 - val_loss: 0.6198 - val_accuracy: 0.6546\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.6287 - accuracy: 0.6389 - val_loss: 0.6416 - val_accuracy: 0.6473\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 4s 170ms/step - loss: 0.6297 - accuracy: 0.6322 - val_loss: 0.6302 - val_accuracy: 0.6546\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.6263 - accuracy: 0.6431 - val_loss: 0.6185 - val_accuracy: 0.6594\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.6234 - accuracy: 0.6479 - val_loss: 0.6179 - val_accuracy: 0.6522\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 5s 177ms/step - loss: 0.6257 - accuracy: 0.6443 - val_loss: 0.6155 - val_accuracy: 0.6473\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 5s 179ms/step - loss: 0.6240 - accuracy: 0.6516 - val_loss: 0.6159 - val_accuracy: 0.6425\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 5s 178ms/step - loss: 0.6208 - accuracy: 0.6588 - val_loss: 0.6150 - val_accuracy: 0.6594\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 4s 170ms/step - loss: 0.6197 - accuracy: 0.6576 - val_loss: 0.6140 - val_accuracy: 0.6473\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.6222 - accuracy: 0.6467 - val_loss: 0.6267 - val_accuracy: 0.6570\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 4s 154ms/step - loss: 0.6184 - accuracy: 0.6534 - val_loss: 0.6100 - val_accuracy: 0.6594\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 4s 157ms/step - loss: 0.6195 - accuracy: 0.6473 - val_loss: 0.6179 - val_accuracy: 0.6594\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 4s 172ms/step - loss: 0.6170 - accuracy: 0.6461 - val_loss: 0.6150 - val_accuracy: 0.6594\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 4s 173ms/step - loss: 0.6128 - accuracy: 0.6558 - val_loss: 0.6078 - val_accuracy: 0.6570\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 5s 182ms/step - loss: 0.6176 - accuracy: 0.6443 - val_loss: 0.6098 - val_accuracy: 0.6425\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.6139 - accuracy: 0.6516 - val_loss: 0.6046 - val_accuracy: 0.6691\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 4s 152ms/step - loss: 0.6122 - accuracy: 0.6504 - val_loss: 0.6016 - val_accuracy: 0.6739\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 4s 152ms/step - loss: 0.6028 - accuracy: 0.6630 - val_loss: 0.6225 - val_accuracy: 0.6401\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 4s 154ms/step - loss: 0.6130 - accuracy: 0.6473 - val_loss: 0.6054 - val_accuracy: 0.6643\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.6119 - accuracy: 0.6618 - val_loss: 0.6223 - val_accuracy: 0.6546\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 4s 168ms/step - loss: 0.6123 - accuracy: 0.6564 - val_loss: 0.6014 - val_accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.6068 - accuracy: 0.6733 - val_loss: 0.6135 - val_accuracy: 0.6401\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.6120 - accuracy: 0.6534 - val_loss: 0.5999 - val_accuracy: 0.6812\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 4s 158ms/step - loss: 0.6040 - accuracy: 0.6630 - val_loss: 0.6095 - val_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 4s 156ms/step - loss: 0.6040 - accuracy: 0.6643 - val_loss: 0.5970 - val_accuracy: 0.6594\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 4s 156ms/step - loss: 0.6022 - accuracy: 0.6661 - val_loss: 0.5974 - val_accuracy: 0.6570\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 4s 153ms/step - loss: 0.6048 - accuracy: 0.6564 - val_loss: 0.6055 - val_accuracy: 0.6715\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 4s 155ms/step - loss: 0.6055 - accuracy: 0.6606 - val_loss: 0.5984 - val_accuracy: 0.6594\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.5978 - accuracy: 0.6612 - val_loss: 0.5975 - val_accuracy: 0.6763\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.6035 - accuracy: 0.6655 - val_loss: 0.6135 - val_accuracy: 0.6570\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 5s 179ms/step - loss: 0.6021 - accuracy: 0.6624 - val_loss: 0.6048 - val_accuracy: 0.6908\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 4s 172ms/step - loss: 0.6038 - accuracy: 0.6624 - val_loss: 0.5923 - val_accuracy: 0.6618\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 4s 171ms/step - loss: 0.6025 - accuracy: 0.6564 - val_loss: 0.5975 - val_accuracy: 0.6667\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5991 - accuracy: 0.6739 - val_loss: 0.6039 - val_accuracy: 0.6667\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.5934 - accuracy: 0.6679 - val_loss: 0.5933 - val_accuracy: 0.6812\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.5963 - accuracy: 0.6649 - val_loss: 0.5964 - val_accuracy: 0.6594\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.5970 - accuracy: 0.6679 - val_loss: 0.5973 - val_accuracy: 0.6618\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.6034 - accuracy: 0.6618 - val_loss: 0.6055 - val_accuracy: 0.6570\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.5990 - accuracy: 0.6618 - val_loss: 0.5862 - val_accuracy: 0.6643\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 4s 158ms/step - loss: 0.5884 - accuracy: 0.6769 - val_loss: 0.5940 - val_accuracy: 0.6594\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.5976 - accuracy: 0.6649 - val_loss: 0.5924 - val_accuracy: 0.6787\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 4s 172ms/step - loss: 0.5956 - accuracy: 0.6649 - val_loss: 0.5854 - val_accuracy: 0.6763\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 5s 182ms/step - loss: 0.5881 - accuracy: 0.6800 - val_loss: 0.5860 - val_accuracy: 0.6643\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 5s 180ms/step - loss: 0.5939 - accuracy: 0.6739 - val_loss: 0.5767 - val_accuracy: 0.6981\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 5s 173ms/step - loss: 0.5944 - accuracy: 0.6836 - val_loss: 0.5798 - val_accuracy: 0.7053\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.5924 - accuracy: 0.6691 - val_loss: 0.5788 - val_accuracy: 0.6957\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 4s 158ms/step - loss: 0.5918 - accuracy: 0.6661 - val_loss: 0.5835 - val_accuracy: 0.6643\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 4s 158ms/step - loss: 0.5981 - accuracy: 0.6697 - val_loss: 0.5915 - val_accuracy: 0.6570\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.5842 - accuracy: 0.6769 - val_loss: 0.5815 - val_accuracy: 0.6787\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.5883 - accuracy: 0.6727 - val_loss: 0.5909 - val_accuracy: 0.7005\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 4s 173ms/step - loss: 0.5863 - accuracy: 0.6733 - val_loss: 0.5846 - val_accuracy: 0.6763\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 5s 182ms/step - loss: 0.5876 - accuracy: 0.6757 - val_loss: 0.5819 - val_accuracy: 0.6715\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 4s 172ms/step - loss: 0.5778 - accuracy: 0.6812 - val_loss: 0.5998 - val_accuracy: 0.6739\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5923 - accuracy: 0.6697 - val_loss: 0.5759 - val_accuracy: 0.6836\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 4s 169ms/step - loss: 0.5798 - accuracy: 0.6842 - val_loss: 0.5695 - val_accuracy: 0.7198\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5820 - accuracy: 0.6842 - val_loss: 0.5714 - val_accuracy: 0.7053\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 4s 160ms/step - loss: 0.5695 - accuracy: 0.6957 - val_loss: 0.5747 - val_accuracy: 0.7053\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 4s 168ms/step - loss: 0.5823 - accuracy: 0.6836 - val_loss: 0.5795 - val_accuracy: 0.6932\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5821 - accuracy: 0.6787 - val_loss: 0.5714 - val_accuracy: 0.6836\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 4s 173ms/step - loss: 0.5763 - accuracy: 0.6848 - val_loss: 0.5728 - val_accuracy: 0.6957\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 5s 175ms/step - loss: 0.5781 - accuracy: 0.6836 - val_loss: 0.5712 - val_accuracy: 0.7077\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 5s 183ms/step - loss: 0.5729 - accuracy: 0.6860 - val_loss: 0.5824 - val_accuracy: 0.6908\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 4s 171ms/step - loss: 0.5801 - accuracy: 0.6818 - val_loss: 0.5768 - val_accuracy: 0.6836\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 5s 182ms/step - loss: 0.5678 - accuracy: 0.6993 - val_loss: 0.5925 - val_accuracy: 0.6957\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5730 - accuracy: 0.6969 - val_loss: 0.5704 - val_accuracy: 0.6981\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 4s 161ms/step - loss: 0.5673 - accuracy: 0.6872 - val_loss: 0.5658 - val_accuracy: 0.6957\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 4s 162ms/step - loss: 0.5725 - accuracy: 0.6896 - val_loss: 0.5763 - val_accuracy: 0.7126\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 4s 171ms/step - loss: 0.5748 - accuracy: 0.6872 - val_loss: 0.5656 - val_accuracy: 0.7029\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 5s 177ms/step - loss: 0.5744 - accuracy: 0.6854 - val_loss: 0.5602 - val_accuracy: 0.7101\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 5s 178ms/step - loss: 0.5693 - accuracy: 0.7017 - val_loss: 0.5673 - val_accuracy: 0.6957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100\n",
      "26/26 [==============================] - 5s 172ms/step - loss: 0.5721 - accuracy: 0.6890 - val_loss: 0.5741 - val_accuracy: 0.6836\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 4s 166ms/step - loss: 0.5738 - accuracy: 0.6914 - val_loss: 0.5695 - val_accuracy: 0.7150\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.5691 - accuracy: 0.6890 - val_loss: 0.5670 - val_accuracy: 0.7029\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5614 - accuracy: 0.7065 - val_loss: 0.5671 - val_accuracy: 0.7029\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 4s 167ms/step - loss: 0.5730 - accuracy: 0.6969 - val_loss: 0.5802 - val_accuracy: 0.7174\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 5s 174ms/step - loss: 0.5717 - accuracy: 0.6884 - val_loss: 0.5652 - val_accuracy: 0.7005\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 5s 184ms/step - loss: 0.5662 - accuracy: 0.7011 - val_loss: 0.5836 - val_accuracy: 0.7029\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 4s 165ms/step - loss: 0.5652 - accuracy: 0.7011 - val_loss: 0.6117 - val_accuracy: 0.6787\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 4s 157ms/step - loss: 0.5603 - accuracy: 0.7083 - val_loss: 0.5837 - val_accuracy: 0.7150\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 4s 159ms/step - loss: 0.5578 - accuracy: 0.7150 - val_loss: 0.6011 - val_accuracy: 0.7053\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 4s 164ms/step - loss: 0.5647 - accuracy: 0.7023 - val_loss: 0.5664 - val_accuracy: 0.7053\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 5s 175ms/step - loss: 0.5676 - accuracy: 0.6944 - val_loss: 0.5694 - val_accuracy: 0.7077\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 5s 178ms/step - loss: 0.5671 - accuracy: 0.7011 - val_loss: 0.5747 - val_accuracy: 0.7174\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 5s 174ms/step - loss: 0.5577 - accuracy: 0.7132 - val_loss: 0.5605 - val_accuracy: 0.7029\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 5s 187ms/step - loss: 0.5662 - accuracy: 0.7065 - val_loss: 0.5696 - val_accuracy: 0.6981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181b2a4f0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = tf.keras.layers.Input(shape=(deep_features_train['ECG_features_C'].shape[1], deep_features_train['ECG_features_C'].shape[2]))\n",
    "inp2 = tf.keras.layers.Input(shape=(deep_features_train['ECG_features_T'].shape[1], deep_features_train['ECG_features_T'].shape[2]))\n",
    "inp3 = tf.keras.layers.Input(shape=(handcrafted_features_train['ECG_features'].shape[1], handcrafted_features_train['ECG_features'].shape[2]))\n",
    "inp4 = tf.keras.layers.Input(shape=(handcrafted_features_train['GSR_features'].shape[1], handcrafted_features_train['GSR_features'].shape[2]))\n",
    "\n",
    "n_units = 32\n",
    "n_batch = 64 \n",
    "recurrent_dropout = 0.5\n",
    "dropout = 0.2\n",
    "optimizer = 'rmsprop'\n",
    "\n",
    "two_layers = True\n",
    "three_layers = True\n",
    "\n",
    "#x = tf.keras.layers.Masking(mask_value=0.)(inp1)\n",
    "#x = tf.keras.layers.GRU(n_units,recurrent_dropout=recurrent_dropout)(x)\n",
    "#x = tf.keras.layers.Dropout(dropout)(x)\n",
    "#x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "\n",
    "x = tf.keras.layers.Masking(mask_value=0.)(inp1)\n",
    "x = tf.keras.layers.GRU(n_units, dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=two_layers or three_layers)(x)\n",
    "if two_layers or three_layers:\n",
    "    x = tf.keras.layers.GRU(int(n_units/2), dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=three_layers)(x)\n",
    "if three_layers:\n",
    "    x = tf.keras.layers.GRU(int(n_units/4), dropout = dropout, recurrent_dropout=recurrent_dropout)(x)\n",
    "\n",
    "y = tf.keras.layers.Masking(mask_value=0.)(inp2)\n",
    "y = tf.keras.layers.GRU(n_units, dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=two_layers or three_layers)(y)\n",
    "if two_layers or three_layers:\n",
    "    y = tf.keras.layers.GRU(int(n_units/2), dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=three_layers)(y)\n",
    "if three_layers:\n",
    "    y = tf.keras.layers.GRU(int(n_units/4), dropout = dropout, recurrent_dropout=recurrent_dropout)(y)\n",
    "\n",
    "z = tf.keras.layers.Masking(mask_value=0.)(inp3)\n",
    "z = tf.keras.layers.GRU(n_units, dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=two_layers or three_layers)(z)\n",
    "if two_layers or three_layers:\n",
    "    z = tf.keras.layers.GRU(int(n_units/2), dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=three_layers)(z)\n",
    "if three_layers:\n",
    "    z = tf.keras.layers.GRU(int(n_units/4), dropout = dropout, recurrent_dropout=recurrent_dropout)(z)\n",
    "\n",
    "w = tf.keras.layers.Masking(mask_value=0.)(inp4)\n",
    "w = tf.keras.layers.GRU(n_units, dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=two_layers or three_layers)(inp4)\n",
    "if two_layers or three_layers:\n",
    "    w = tf.keras.layers.GRU(int(n_units/2), dropout = dropout, recurrent_dropout=recurrent_dropout,return_sequences=three_layers)(w)\n",
    "if three_layers:\n",
    "    w = tf.keras.layers.GRU(int(n_units/4), dropout = dropout, recurrent_dropout=recurrent_dropout)(w)\n",
    "#w = tf.keras.layers.Dense(1)(w)\n",
    "\n",
    "c = tf.keras.layers.Concatenate()([\n",
    "    #x,\n",
    "    y,\n",
    "    z, \n",
    "    w\n",
    "])\n",
    "\n",
    "c =  tf.keras.layers.Dense(16, activation='sigmoid')(c)\n",
    "\n",
    "out =  tf.keras.layers.Dense(1, activation='sigmoid')(c)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[\n",
    "        #inp1,\n",
    "        inp2,\n",
    "        inp3,\n",
    "        inp4\n",
    "        ],\n",
    "        outputs=out)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x=[\n",
    "            #deep_features_train['ECG_features_C'],\n",
    "            deep_features_train['ECG_features_T'],\n",
    "            handcrafted_features_train['ECG_features'],\n",
    "            handcrafted_features_train['GSR_features']\n",
    "            ], \n",
    "          y=y_train, \n",
    "          epochs=100,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          batch_size = n_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fc4d14e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-24d4d76ff263cfde\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-24d4d76ff263cfde\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9998e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_82 (InputLayer)          [(None, 60, 64)]     0           []                               \n",
      "                                                                                                  \n",
      " input_83 (InputLayer)          [(None, 60, 8)]      0           []                               \n",
      "                                                                                                  \n",
      " masking_56 (Masking)           (None, 60, 64)       0           ['input_82[0][0]']               \n",
      "                                                                                                  \n",
      " masking_57 (Masking)           (None, 60, 8)        0           ['input_83[0][0]']               \n",
      "                                                                                                  \n",
      " input_84 (InputLayer)          [(None, 60, 12)]     0           []                               \n",
      "                                                                                                  \n",
      " gru_91 (GRU)                   (None, 60, 32)       9408        ['masking_56[0][0]']             \n",
      "                                                                                                  \n",
      " gru_94 (GRU)                   (None, 60, 32)       4032        ['masking_57[0][0]']             \n",
      "                                                                                                  \n",
      " gru_97 (GRU)                   (None, 60, 32)       4416        ['input_84[0][0]']               \n",
      "                                                                                                  \n",
      " gru_92 (GRU)                   (None, 60, 16)       2400        ['gru_91[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_95 (GRU)                   (None, 60, 16)       2400        ['gru_94[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_98 (GRU)                   (None, 60, 16)       2400        ['gru_97[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_93 (GRU)                   (None, 8)            624         ['gru_92[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_96 (GRU)                   (None, 8)            624         ['gru_95[0][0]']                 \n",
      "                                                                                                  \n",
      " gru_99 (GRU)                   (None, 8)            624         ['gru_98[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate_12 (Concatenate)   (None, 24)           0           ['gru_93[0][0]',                 \n",
      "                                                                  'gru_96[0][0]',                 \n",
      "                                                                  'gru_99[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 16)           400         ['concatenate_12[0][0]']         \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 1)            17          ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,345\n",
      "Trainable params: 27,345\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "33/33 [==============================] - 21s 131ms/step - loss: 0.5679 - accuracy: 0.7014\n",
      "Epoch 2/30\n",
      "33/33 [==============================] - 5s 138ms/step - loss: 0.5575 - accuracy: 0.7116\n",
      "Epoch 3/30\n",
      "33/33 [==============================] - 4s 131ms/step - loss: 0.5614 - accuracy: 0.6995\n",
      "Epoch 4/30\n",
      "33/33 [==============================] - 4s 136ms/step - loss: 0.5667 - accuracy: 0.7034\n",
      "Epoch 5/30\n",
      "33/33 [==============================] - 4s 136ms/step - loss: 0.5591 - accuracy: 0.7092\n",
      "Epoch 6/30\n",
      "33/33 [==============================] - 6s 186ms/step - loss: 0.5575 - accuracy: 0.7048\n",
      "Epoch 7/30\n",
      "33/33 [==============================] - 7s 197ms/step - loss: 0.5577 - accuracy: 0.7010\n",
      "Epoch 8/30\n",
      "33/33 [==============================] - 6s 170ms/step - loss: 0.5555 - accuracy: 0.7087\n",
      "Epoch 9/30\n",
      "33/33 [==============================] - 5s 143ms/step - loss: 0.5630 - accuracy: 0.7029\n",
      "Epoch 10/30\n",
      "33/33 [==============================] - 5s 141ms/step - loss: 0.5546 - accuracy: 0.7101\n",
      "Epoch 11/30\n",
      "33/33 [==============================] - 5s 144ms/step - loss: 0.5586 - accuracy: 0.7111\n",
      "Epoch 12/30\n",
      "33/33 [==============================] - 5s 152ms/step - loss: 0.5557 - accuracy: 0.7039\n",
      "Epoch 13/30\n",
      "33/33 [==============================] - 5s 159ms/step - loss: 0.5580 - accuracy: 0.7058\n",
      "Epoch 14/30\n",
      "33/33 [==============================] - 5s 154ms/step - loss: 0.5445 - accuracy: 0.7155\n",
      "Epoch 15/30\n",
      "33/33 [==============================] - 5s 143ms/step - loss: 0.5580 - accuracy: 0.7082\n",
      "Epoch 16/30\n",
      "33/33 [==============================] - 5s 144ms/step - loss: 0.5507 - accuracy: 0.7039\n",
      "Epoch 17/30\n",
      "33/33 [==============================] - 5s 145ms/step - loss: 0.5522 - accuracy: 0.6990\n",
      "Epoch 18/30\n",
      "33/33 [==============================] - 5s 156ms/step - loss: 0.5498 - accuracy: 0.7111\n",
      "Epoch 19/30\n",
      "33/33 [==============================] - 5s 166ms/step - loss: 0.5466 - accuracy: 0.7135\n",
      "Epoch 20/30\n",
      "33/33 [==============================] - 6s 172ms/step - loss: 0.5488 - accuracy: 0.7140\n",
      "Epoch 21/30\n",
      "33/33 [==============================] - 5s 160ms/step - loss: 0.5474 - accuracy: 0.7087\n",
      "Epoch 22/30\n",
      "33/33 [==============================] - 5s 151ms/step - loss: 0.5498 - accuracy: 0.7063\n",
      "Epoch 23/30\n",
      "33/33 [==============================] - 5s 150ms/step - loss: 0.5402 - accuracy: 0.7203\n",
      "Epoch 24/30\n",
      "33/33 [==============================] - 5s 149ms/step - loss: 0.5536 - accuracy: 0.7155\n",
      "Epoch 25/30\n",
      "33/33 [==============================] - 5s 154ms/step - loss: 0.5418 - accuracy: 0.7217\n",
      "Epoch 26/30\n",
      "33/33 [==============================] - 5s 147ms/step - loss: 0.5455 - accuracy: 0.7169\n",
      "Epoch 27/30\n",
      "33/33 [==============================] - 5s 144ms/step - loss: 0.5507 - accuracy: 0.7106\n",
      "Epoch 28/30\n",
      "33/33 [==============================] - 5s 144ms/step - loss: 0.5487 - accuracy: 0.7063\n",
      "Epoch 29/30\n",
      "33/33 [==============================] - 5s 158ms/step - loss: 0.5478 - accuracy: 0.7087\n",
      "Epoch 30/30\n",
      "33/33 [==============================] - 5s 159ms/step - loss: 0.5481 - accuracy: 0.7068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1815a8250>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Model(inputs=[\n",
    "        #inp1,\n",
    "        inp2,\n",
    "        inp3,\n",
    "        inp4\n",
    "        ],\n",
    "        outputs=out)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(x=[\n",
    "            #deep_features_train['ECG_features_C'],\n",
    "            deep_features_train['ECG_features_T'],\n",
    "            handcrafted_features_train['ECG_features'],\n",
    "            handcrafted_features_train['GSR_features']\n",
    "            ], \n",
    "          y=y_train, \n",
    "          epochs=50,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          batch_size = n_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3f53f",
   "metadata": {},
   "source": [
    "# Make and save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a9e99542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 21ms/step\n",
      "[[0.81311905]\n",
      " [0.8748807 ]\n",
      " [0.73365885]\n",
      " [0.9043902 ]\n",
      " [0.78710985]\n",
      " [0.85136294]\n",
      " [0.64858943]\n",
      " [0.6342362 ]\n",
      " [0.71090853]\n",
      " [0.68981034]\n",
      " [0.69976306]\n",
      " [0.54497564]\n",
      " [0.62285966]\n",
      " [0.71511626]\n",
      " [0.510514  ]\n",
      " [0.63853467]\n",
      " [0.5920097 ]\n",
      " [0.6836758 ]\n",
      " [0.83730686]\n",
      " [0.89214146]\n",
      " [0.73632735]\n",
      " [0.6996339 ]\n",
      " [0.8898366 ]\n",
      " [0.9459672 ]\n",
      " [0.42786235]\n",
      " [0.41356122]\n",
      " [0.690152  ]\n",
      " [0.7882622 ]\n",
      " [0.5336361 ]\n",
      " [0.59279907]\n",
      " [0.78511316]\n",
      " [0.59617263]\n",
      " [0.6505169 ]\n",
      " [0.5592191 ]\n",
      " [0.7501    ]\n",
      " [0.5602623 ]\n",
      " [0.72314435]\n",
      " [0.6473776 ]\n",
      " [0.58208126]\n",
      " [0.7021614 ]\n",
      " [0.7473672 ]\n",
      " [0.73853016]\n",
      " [0.9412117 ]\n",
      " [0.5183402 ]\n",
      " [0.64050627]\n",
      " [0.7015032 ]\n",
      " [0.67723143]\n",
      " [0.62363523]\n",
      " [0.6696618 ]\n",
      " [0.63286847]\n",
      " [0.5191433 ]\n",
      " [0.86951226]\n",
      " [0.68765587]\n",
      " [0.65486425]\n",
      " [0.5714667 ]\n",
      " [0.73552924]\n",
      " [0.73083216]\n",
      " [0.6817647 ]\n",
      " [0.6254139 ]\n",
      " [0.6869535 ]\n",
      " [0.8421723 ]\n",
      " [0.3485261 ]\n",
      " [0.58015203]\n",
      " [0.92588395]\n",
      " [0.7173929 ]\n",
      " [0.6431098 ]\n",
      " [0.80443305]\n",
      " [0.61882955]\n",
      " [0.64951104]\n",
      " [0.7906356 ]\n",
      " [0.6345765 ]\n",
      " [0.55308187]\n",
      " [0.6423022 ]\n",
      " [0.7105187 ]\n",
      " [0.70922405]\n",
      " [0.6701087 ]\n",
      " [0.7026126 ]\n",
      " [0.727419  ]\n",
      " [0.8973733 ]\n",
      " [0.5970911 ]\n",
      " [0.594143  ]\n",
      " [0.75644183]\n",
      " [0.70691377]\n",
      " [0.9442611 ]\n",
      " [0.63181543]\n",
      " [0.74205786]\n",
      " [0.8053555 ]\n",
      " [0.9028487 ]\n",
      " [0.6996665 ]\n",
      " [0.44786665]\n",
      " [0.7879931 ]\n",
      " [0.39416248]\n",
      " [0.5235324 ]\n",
      " [0.8937344 ]\n",
      " [0.5973851 ]\n",
      " [0.5437782 ]\n",
      " [0.66254073]\n",
      " [0.66986257]\n",
      " [0.57073885]\n",
      " [0.6540621 ]\n",
      " [0.6329965 ]\n",
      " [0.64727587]\n",
      " [0.61702394]\n",
      " [0.7578404 ]\n",
      " [0.628849  ]\n",
      " [0.70917845]\n",
      " [0.8128913 ]\n",
      " [0.81088454]\n",
      " [0.81925356]\n",
      " [0.48773095]\n",
      " [0.68567497]\n",
      " [0.858511  ]\n",
      " [0.92367196]\n",
      " [0.29089645]\n",
      " [0.53648853]\n",
      " [0.71756625]\n",
      " [0.71200234]\n",
      " [0.89301246]\n",
      " [0.7761426 ]\n",
      " [0.41035706]\n",
      " [0.8182493 ]\n",
      " [0.8647181 ]\n",
      " [0.5944702 ]\n",
      " [0.5761093 ]\n",
      " [0.66257757]\n",
      " [0.75341403]\n",
      " [0.8305902 ]\n",
      " [0.81362176]\n",
      " [0.86044866]\n",
      " [0.514446  ]\n",
      " [0.6617181 ]\n",
      " [0.69479126]\n",
      " [0.5899671 ]\n",
      " [0.64068085]\n",
      " [0.6866107 ]\n",
      " [0.83768547]\n",
      " [0.6782243 ]\n",
      " [0.82086813]\n",
      " [0.8780417 ]\n",
      " [0.7875527 ]\n",
      " [0.29326105]\n",
      " [0.7528272 ]\n",
      " [0.7898329 ]\n",
      " [0.48437396]\n",
      " [0.6407882 ]\n",
      " [0.7746387 ]\n",
      " [0.70512104]\n",
      " [0.53934026]\n",
      " [0.508351  ]\n",
      " [0.5187506 ]\n",
      " [0.42095473]\n",
      " [0.44633198]\n",
      " [0.29339328]\n",
      " [0.46752968]\n",
      " [0.18447523]\n",
      " [0.270514  ]\n",
      " [0.384221  ]\n",
      " [0.42565805]\n",
      " [0.28234518]\n",
      " [0.42108953]\n",
      " [0.24962527]\n",
      " [0.25703967]\n",
      " [0.29118213]\n",
      " [0.2726827 ]\n",
      " [0.26503065]\n",
      " [0.38705623]\n",
      " [0.25300768]\n",
      " [0.51145303]\n",
      " [0.41641384]\n",
      " [0.52591985]\n",
      " [0.534363  ]\n",
      " [0.50320363]\n",
      " [0.48649633]\n",
      " [0.25739604]\n",
      " [0.35309592]\n",
      " [0.37678304]\n",
      " [0.5172686 ]\n",
      " [0.5219074 ]\n",
      " [0.4972548 ]\n",
      " [0.51721364]\n",
      " [0.25643733]\n",
      " [0.40485394]\n",
      " [0.4803702 ]\n",
      " [0.39400226]\n",
      " [0.38491675]\n",
      " [0.36463273]\n",
      " [0.46203297]\n",
      " [0.5158835 ]\n",
      " [0.4900907 ]\n",
      " [0.4390913 ]\n",
      " [0.31129488]\n",
      " [0.23078832]\n",
      " [0.2511515 ]\n",
      " [0.45289916]\n",
      " [0.27924243]\n",
      " [0.64355534]\n",
      " [0.46484962]\n",
      " [0.45775932]\n",
      " [0.3020527 ]\n",
      " [0.5053522 ]\n",
      " [0.50816494]\n",
      " [0.35147744]\n",
      " [0.35834783]\n",
      " [0.2581365 ]\n",
      " [0.35036233]\n",
      " [0.2644289 ]\n",
      " [0.28105736]\n",
      " [0.40366977]\n",
      " [0.24700494]\n",
      " [0.31656066]\n",
      " [0.40279317]\n",
      " [0.5488833 ]\n",
      " [0.51419073]\n",
      " [0.5091195 ]\n",
      " [0.5585709 ]\n",
      " [0.5171911 ]\n",
      " [0.56148607]\n",
      " [0.51892257]\n",
      " [0.17782567]\n",
      " [0.17970833]\n",
      " [0.18101728]\n",
      " [0.18022154]\n",
      " [0.1852572 ]\n",
      " [0.17885308]\n",
      " [0.1666863 ]\n",
      " [0.1647655 ]\n",
      " [0.17888042]\n",
      " [0.17637   ]\n",
      " [0.17324151]\n",
      " [0.16385888]\n",
      " [0.1721624 ]\n",
      " [0.15454347]\n",
      " [0.16205142]\n",
      " [0.17867571]\n",
      " [0.16561295]\n",
      " [0.16198112]\n",
      " [0.15276308]\n",
      " [0.1546849 ]\n",
      " [0.15663366]\n",
      " [0.16928901]\n",
      " [0.1599376 ]\n",
      " [0.16336568]\n",
      " [0.1667341 ]\n",
      " [0.15923208]\n",
      " [0.16990578]\n",
      " [0.15817519]\n",
      " [0.1610665 ]\n",
      " [0.15537928]\n",
      " [0.16487601]\n",
      " [0.16009967]\n",
      " [0.17359881]\n",
      " [0.15869276]\n",
      " [0.1564534 ]\n",
      " [0.22670136]\n",
      " [0.40901253]\n",
      " [0.28379387]\n",
      " [0.12696911]\n",
      " [0.24359246]\n",
      " [0.24427518]\n",
      " [0.29825616]\n",
      " [0.2565124 ]\n",
      " [0.35772592]\n",
      " [0.22431928]\n",
      " [0.35254723]\n",
      " [0.24861252]\n",
      " [0.2531071 ]\n",
      " [0.27606958]\n",
      " [0.25935906]\n",
      " [0.29053527]\n",
      " [0.20228383]\n",
      " [0.29257435]\n",
      " [0.28479937]\n",
      " [0.32240337]\n",
      " [0.23692717]\n",
      " [0.24329244]\n",
      " [0.24252242]\n",
      " [0.24322075]\n",
      " [0.23139697]\n",
      " [0.25080547]\n",
      " [0.23691039]\n",
      " [0.25047368]\n",
      " [0.2777189 ]\n",
      " [0.24820651]\n",
      " [0.22547893]\n",
      " [0.21007282]\n",
      " [0.22652021]\n",
      " [0.23015046]\n",
      " [0.23945725]\n",
      " [0.24697404]\n",
      " [0.17838486]\n",
      " [0.23499154]\n",
      " [0.25463995]\n",
      " [0.83021986]\n",
      " [0.79334325]\n",
      " [0.8280142 ]\n",
      " [0.6166592 ]\n",
      " [0.8418026 ]\n",
      " [0.86185306]\n",
      " [0.47841638]\n",
      " [0.8487737 ]\n",
      " [0.84739333]\n",
      " [0.5596149 ]\n",
      " [0.62125194]\n",
      " [0.78961307]\n",
      " [0.9137186 ]\n",
      " [0.91377354]\n",
      " [0.55623406]\n",
      " [0.70492476]\n",
      " [0.41152415]\n",
      " [0.61250675]\n",
      " [0.5744096 ]\n",
      " [0.53520745]\n",
      " [0.5628123 ]\n",
      " [0.77167535]\n",
      " [0.5708233 ]\n",
      " [0.45274618]\n",
      " [0.48858672]\n",
      " [0.66289234]\n",
      " [0.5847883 ]\n",
      " [0.5434536 ]\n",
      " [0.46368152]\n",
      " [0.5101651 ]\n",
      " [0.64774346]\n",
      " [0.7950707 ]\n",
      " [0.7613698 ]\n",
      " [0.63613504]\n",
      " [0.71542454]\n",
      " [0.7573429 ]\n",
      " [0.3506945 ]\n",
      " [0.58930475]\n",
      " [0.45113152]\n",
      " [0.45698076]\n",
      " [0.8676248 ]\n",
      " [0.8657398 ]\n",
      " [0.445432  ]\n",
      " [0.3139675 ]\n",
      " [0.6994681 ]\n",
      " [0.68114   ]\n",
      " [0.69192755]\n",
      " [0.4665668 ]\n",
      " [0.89428306]\n",
      " [0.9054169 ]\n",
      " [0.8665657 ]\n",
      " [0.71520215]\n",
      " [0.14064652]\n",
      " [0.17662792]\n",
      " [0.15799756]\n",
      " [0.16428117]\n",
      " [0.21952069]\n",
      " [0.19171862]\n",
      " [0.39870813]\n",
      " [0.2537072 ]\n",
      " [0.22875798]\n",
      " [0.24359292]\n",
      " [0.23444855]\n",
      " [0.25425482]\n",
      " [0.22164583]\n",
      " [0.2310446 ]\n",
      " [0.23029397]\n",
      " [0.22614   ]\n",
      " [0.23683657]\n",
      " [0.254951  ]\n",
      " [0.24325767]\n",
      " [0.2597952 ]\n",
      " [0.25478297]\n",
      " [0.2498964 ]\n",
      " [0.272083  ]\n",
      " [0.4840066 ]\n",
      " [0.24366702]\n",
      " [0.27495173]\n",
      " [0.25826877]\n",
      " [0.36866525]\n",
      " [0.23638341]\n",
      " [0.25336573]\n",
      " [0.25940225]\n",
      " [0.2459251 ]\n",
      " [0.23712209]\n",
      " [0.2594363 ]\n",
      " [0.2938512 ]\n",
      " [0.2765318 ]\n",
      " [0.27233028]\n",
      " [0.2256802 ]\n",
      " [0.37987113]\n",
      " [0.24513748]\n",
      " [0.24548729]\n",
      " [0.31551886]\n",
      " [0.28065223]\n",
      " [0.34846923]\n",
      " [0.23942162]\n",
      " [0.26122683]\n",
      " [0.37215483]\n",
      " [0.46522036]\n",
      " [0.3215078 ]\n",
      " [0.38930583]\n",
      " [0.47962037]\n",
      " [0.43926677]\n",
      " [0.5451281 ]\n",
      " [0.30973306]\n",
      " [0.34139395]\n",
      " [0.33114347]\n",
      " [0.30982065]\n",
      " [0.395211  ]\n",
      " [0.48948264]\n",
      " [0.2926052 ]\n",
      " [0.28280333]\n",
      " [0.28862986]\n",
      " [0.3202175 ]\n",
      " [0.34757978]\n",
      " [0.4896044 ]\n",
      " [0.2830468 ]\n",
      " [0.31910035]\n",
      " [0.33075187]\n",
      " [0.35440704]\n",
      " [0.39684   ]\n",
      " [0.38620102]\n",
      " [0.2916526 ]\n",
      " [0.4199941 ]\n",
      " [0.40412146]\n",
      " [0.42078468]\n",
      " [0.4393438 ]\n",
      " [0.31874654]\n",
      " [0.7640233 ]\n",
      " [0.86542934]\n",
      " [0.71403074]\n",
      " [0.79628766]\n",
      " [0.7946403 ]\n",
      " [0.8171042 ]\n",
      " [0.84851295]\n",
      " [0.7875091 ]\n",
      " [0.7915686 ]\n",
      " [0.84056485]\n",
      " [0.82618165]\n",
      " [0.7783983 ]\n",
      " [0.82629174]\n",
      " [0.86654735]\n",
      " [0.88492644]\n",
      " [0.83419067]\n",
      " [0.87080586]\n",
      " [0.5278818 ]\n",
      " [0.8112976 ]\n",
      " [0.8771722 ]\n",
      " [0.8854561 ]\n",
      " [0.6419346 ]\n",
      " [0.83299315]\n",
      " [0.679446  ]\n",
      " [0.74739355]\n",
      " [0.6186451 ]\n",
      " [0.74847716]\n",
      " [0.7579071 ]\n",
      " [0.63561076]\n",
      " [0.76466346]\n",
      " [0.87793   ]\n",
      " [0.7517721 ]\n",
      " [0.8274802 ]\n",
      " [0.8535669 ]\n",
      " [0.88966465]\n",
      " [0.8423958 ]\n",
      " [0.87396836]\n",
      " [0.8106322 ]\n",
      " [0.5917054 ]\n",
      " [0.7604973 ]\n",
      " [0.63959754]\n",
      " [0.62342197]\n",
      " [0.87279886]\n",
      " [0.84518695]\n",
      " [0.6587738 ]\n",
      " [0.6000071 ]\n",
      " [0.53257364]\n",
      " [0.5952553 ]\n",
      " [0.5955134 ]\n",
      " [0.6258046 ]\n",
      " [0.7646322 ]\n",
      " [0.7172332 ]\n",
      " [0.8485768 ]\n",
      " [0.6233366 ]\n",
      " [0.63657534]\n",
      " [0.6314439 ]\n",
      " [0.64724016]\n",
      " [0.632127  ]\n",
      " [0.58946687]\n",
      " [0.6449613 ]\n",
      " [0.70853746]\n",
      " [0.6850412 ]\n",
      " [0.54552555]\n",
      " [0.49866274]\n",
      " [0.3077964 ]\n",
      " [0.62352735]\n",
      " [0.87674844]\n",
      " [0.8491993 ]\n",
      " [0.5400656 ]\n",
      " [0.66979885]\n",
      " [0.6245898 ]\n",
      " [0.64202315]\n",
      " [0.31615835]\n",
      " [0.5411624 ]\n",
      " [0.73307383]\n",
      " [0.66715354]\n",
      " [0.6659459 ]\n",
      " [0.6400278 ]\n",
      " [0.28827438]\n",
      " [0.29228684]\n",
      " [0.49998358]\n",
      " [0.42279863]\n",
      " [0.47038016]\n",
      " [0.4767533 ]\n",
      " [0.3173167 ]\n",
      " [0.16188546]\n",
      " [0.47709703]\n",
      " [0.33359212]\n",
      " [0.32583883]\n",
      " [0.4333617 ]\n",
      " [0.35882267]\n",
      " [0.5434273 ]\n",
      " [0.48139793]\n",
      " [0.47517276]\n",
      " [0.35980615]\n",
      " [0.35762295]\n",
      " [0.22877456]\n",
      " [0.21064764]\n",
      " [0.40948403]\n",
      " [0.4138252 ]\n",
      " [0.222776  ]\n",
      " [0.21860057]\n",
      " [0.31044757]\n",
      " [0.37369394]\n",
      " [0.3710363 ]\n",
      " [0.36654896]\n",
      " [0.28435603]\n",
      " [0.38834393]\n",
      " [0.27222073]\n",
      " [0.20442946]\n",
      " [0.51716584]\n",
      " [0.3283801 ]\n",
      " [0.376086  ]\n",
      " [0.5470834 ]\n",
      " [0.5424839 ]\n",
      " [0.49191907]\n",
      " [0.51543456]\n",
      " [0.52614707]\n",
      " [0.39264914]\n",
      " [0.31944925]\n",
      " [0.41623583]\n",
      " [0.39801696]\n",
      " [0.5039934 ]\n",
      " [0.5124333 ]\n",
      " [0.5075273 ]\n",
      " [0.51026666]\n",
      " [0.4522006 ]\n",
      " [0.49075878]\n",
      " [0.40027922]\n",
      " [0.16750582]\n",
      " [0.42499375]\n",
      " [0.39293924]\n",
      " [0.44790226]\n",
      " [0.5300419 ]\n",
      " [0.53321403]\n",
      " [0.5049408 ]\n",
      " [0.45913196]\n",
      " [0.49741375]\n",
      " [0.21238853]\n",
      " [0.33423707]\n",
      " [0.45297158]\n",
      " [0.39086175]\n",
      " [0.21415266]\n",
      " [0.4828516 ]\n",
      " [0.48761314]\n",
      " [0.52442676]\n",
      " [0.27084038]\n",
      " [0.2599124 ]\n",
      " [0.22412086]\n",
      " [0.11999851]\n",
      " [0.29310912]\n",
      " [0.36772203]\n",
      " [0.2908673 ]\n",
      " [0.37492445]\n",
      " [0.3863097 ]\n",
      " [0.2290059 ]\n",
      " [0.41804883]\n",
      " [0.26461628]\n",
      " [0.1707647 ]\n",
      " [0.22119848]\n",
      " [0.42809415]\n",
      " [0.32418954]\n",
      " [0.43534446]\n",
      " [0.49320936]\n",
      " [0.8284154 ]\n",
      " [0.5303021 ]\n",
      " [0.5312551 ]\n",
      " [0.55276763]\n",
      " [0.6499751 ]\n",
      " [0.565603  ]\n",
      " [0.42344248]\n",
      " [0.22351271]\n",
      " [0.48106936]\n",
      " [0.24973126]\n",
      " [0.32308075]\n",
      " [0.42310065]\n",
      " [0.7649469 ]\n",
      " [0.5557791 ]\n",
      " [0.7653472 ]\n",
      " [0.68761635]\n",
      " [0.38144964]\n",
      " [0.5372857 ]\n",
      " [0.27107275]\n",
      " [0.56558275]\n",
      " [0.4047733 ]\n",
      " [0.39240882]\n",
      " [0.27877077]\n",
      " [0.21676014]\n",
      " [0.21265498]\n",
      " [0.3090886 ]\n",
      " [0.2733574 ]\n",
      " [0.29962316]\n",
      " [0.22920023]\n",
      " [0.27601272]\n",
      " [0.44926375]\n",
      " [0.27029026]\n",
      " [0.28615627]\n",
      " [0.65285987]\n",
      " [0.25485188]\n",
      " [0.5226822 ]\n",
      " [0.40034047]\n",
      " [0.26215056]\n",
      " [0.2510818 ]\n",
      " [0.19014911]\n",
      " [0.5822885 ]\n",
      " [0.5352122 ]\n",
      " [0.20595147]\n",
      " [0.25217983]\n",
      " [0.23244195]\n",
      " [0.2654493 ]\n",
      " [0.40406162]\n",
      " [0.87925786]\n",
      " [0.5471705 ]\n",
      " [0.68217313]\n",
      " [0.35541832]\n",
      " [0.45344907]\n",
      " [0.39106852]\n",
      " [0.26084974]\n",
      " [0.40749362]\n",
      " [0.37529942]\n",
      " [0.2707536 ]\n",
      " [0.24922149]\n",
      " [0.37673715]\n",
      " [0.22500162]\n",
      " [0.21762297]\n",
      " [0.6027107 ]\n",
      " [0.6198511 ]\n",
      " [0.23482   ]\n",
      " [0.29456314]\n",
      " [0.663507  ]\n",
      " [0.44913492]\n",
      " [0.44186983]\n",
      " [0.5856618 ]\n",
      " [0.6404999 ]\n",
      " [0.53843606]\n",
      " [0.5928    ]\n",
      " [0.57852167]\n",
      " [0.8791467 ]\n",
      " [0.9071269 ]\n",
      " [0.8908504 ]\n",
      " [0.8991134 ]\n",
      " [0.8851923 ]\n",
      " [0.87324715]\n",
      " [0.8999565 ]\n",
      " [0.9068455 ]\n",
      " [0.89814174]\n",
      " [0.86610675]\n",
      " [0.331471  ]\n",
      " [0.12410705]\n",
      " [0.34889373]\n",
      " [0.34738532]\n",
      " [0.14737861]\n",
      " [0.3156412 ]\n",
      " [0.31050965]\n",
      " [0.28747845]\n",
      " [0.16244711]\n",
      " [0.1545146 ]\n",
      " [0.3073793 ]\n",
      " [0.37739584]\n",
      " [0.35404772]\n",
      " [0.25206393]\n",
      " [0.34339967]\n",
      " [0.31795117]\n",
      " [0.38181543]\n",
      " [0.44306225]\n",
      " [0.18641543]\n",
      " [0.22056201]\n",
      " [0.23142363]\n",
      " [0.27381653]\n",
      " [0.20311074]\n",
      " [0.3433213 ]\n",
      " [0.18317205]\n",
      " [0.19644512]\n",
      " [0.1483779 ]\n",
      " [0.18624319]\n",
      " [0.1760132 ]\n",
      " [0.17196678]\n",
      " [0.18891989]\n",
      " [0.13958366]\n",
      " [0.15791175]\n",
      " [0.2093617 ]\n",
      " [0.11922202]\n",
      " [0.14722726]\n",
      " [0.2206597 ]\n",
      " [0.18725827]\n",
      " [0.14177957]\n",
      " [0.16555186]\n",
      " [0.20258634]\n",
      " [0.18502717]\n",
      " [0.1632748 ]\n",
      " [0.15636036]\n",
      " [0.21012454]\n",
      " [0.12788403]\n",
      " [0.17785756]\n",
      " [0.30578643]\n",
      " [0.35462937]\n",
      " [0.19788058]\n",
      " [0.2370658 ]\n",
      " [0.18377855]\n",
      " [0.11437903]\n",
      " [0.11559594]\n",
      " [0.11138922]\n",
      " [0.16960287]\n",
      " [0.11094914]\n",
      " [0.47951406]\n",
      " [0.12265425]\n",
      " [0.2249876 ]\n",
      " [0.35932043]\n",
      " [0.27670416]\n",
      " [0.31778952]\n",
      " [0.33858338]\n",
      " [0.20425567]\n",
      " [0.2112465 ]\n",
      " [0.12532018]\n",
      " [0.25275767]\n",
      " [0.36669806]\n",
      " [0.15411481]\n",
      " [0.2993233 ]\n",
      " [0.27410594]\n",
      " [0.38670418]\n",
      " [0.35467336]\n",
      " [0.15251705]\n",
      " [0.20468079]\n",
      " [0.21465154]\n",
      " [0.35156575]\n",
      " [0.45100456]\n",
      " [0.6629037 ]\n",
      " [0.27806652]\n",
      " [0.25123167]\n",
      " [0.3637695 ]\n",
      " [0.9005123 ]\n",
      " [0.8977716 ]\n",
      " [0.89689535]\n",
      " [0.86842734]\n",
      " [0.84388566]\n",
      " [0.7193801 ]\n",
      " [0.8706062 ]\n",
      " [0.8887807 ]\n",
      " [0.8627242 ]\n",
      " [0.9013572 ]\n",
      " [0.6909537 ]\n",
      " [0.68242425]\n",
      " [0.46731856]\n",
      " [0.653135  ]\n",
      " [0.23356594]\n",
      " [0.2880948 ]\n",
      " [0.2548172 ]\n",
      " [0.22845574]\n",
      " [0.23548721]\n",
      " [0.6656727 ]\n",
      " [0.56094646]\n",
      " [0.31029472]\n",
      " [0.43687627]\n",
      " [0.45913088]\n",
      " [0.5228586 ]\n",
      " [0.6267391 ]\n",
      " [0.6573694 ]\n",
      " [0.56310946]\n",
      " [0.6564745 ]\n",
      " [0.66839004]\n",
      " [0.6767909 ]\n",
      " [0.6767145 ]\n",
      " [0.6317955 ]\n",
      " [0.66115814]\n",
      " [0.66600907]\n",
      " [0.6601938 ]\n",
      " [0.68828994]\n",
      " [0.65158916]\n",
      " [0.6553954 ]\n",
      " [0.7196867 ]\n",
      " [0.5819137 ]\n",
      " [0.79893756]\n",
      " [0.5289906 ]\n",
      " [0.626356  ]\n",
      " [0.616395  ]\n",
      " [0.68502665]\n",
      " [0.6733664 ]\n",
      " [0.6744707 ]\n",
      " [0.67788297]\n",
      " [0.6612666 ]\n",
      " [0.6492981 ]\n",
      " [0.6605472 ]\n",
      " [0.68603045]\n",
      " [0.68509936]\n",
      " [0.6794103 ]\n",
      " [0.6655576 ]\n",
      " [0.69258714]\n",
      " [0.68098104]\n",
      " [0.67864835]\n",
      " [0.6851439 ]\n",
      " [0.67947274]\n",
      " [0.68672544]\n",
      " [0.673427  ]\n",
      " [0.68151385]\n",
      " [0.6880364 ]\n",
      " [0.676156  ]\n",
      " [0.63940835]\n",
      " [0.6765596 ]\n",
      " [0.5619743 ]\n",
      " [0.66244036]\n",
      " [0.6656324 ]\n",
      " [0.2795101 ]\n",
      " [0.18792301]\n",
      " [0.23447241]\n",
      " [0.36549157]\n",
      " [0.3690541 ]\n",
      " [0.33464321]\n",
      " [0.38058555]\n",
      " [0.2078186 ]\n",
      " [0.24561279]\n",
      " [0.27959847]\n",
      " [0.28481865]\n",
      " [0.2662508 ]\n",
      " [0.29037413]\n",
      " [0.38025513]\n",
      " [0.33884728]\n",
      " [0.25968564]\n",
      " [0.21203089]\n",
      " [0.21429816]\n",
      " [0.21385515]\n",
      " [0.15236507]\n",
      " [0.25966707]\n",
      " [0.36876127]\n",
      " [0.2711745 ]\n",
      " [0.24371922]\n",
      " [0.23423849]\n",
      " [0.24027795]\n",
      " [0.23620507]\n",
      " [0.25437477]\n",
      " [0.2536365 ]\n",
      " [0.24555111]\n",
      " [0.26898673]\n",
      " [0.24513094]\n",
      " [0.23506641]\n",
      " [0.28176484]\n",
      " [0.24489208]\n",
      " [0.38029566]\n",
      " [0.23624192]\n",
      " [0.25262776]\n",
      " [0.25794414]\n",
      " [0.2793963 ]\n",
      " [0.49258336]\n",
      " [0.33061522]\n",
      " [0.25072867]\n",
      " [0.26792812]\n",
      " [0.21674554]\n",
      " [0.2526926 ]\n",
      " [0.25058562]\n",
      " [0.2476923 ]\n",
      " [0.24666743]\n",
      " [0.2566499 ]\n",
      " [0.24945176]\n",
      " [0.23904526]\n",
      " [0.3279236 ]\n",
      " [0.23382203]\n",
      " [0.25823945]\n",
      " [0.25158742]\n",
      " [0.23500112]\n",
      " [0.22208577]\n",
      " [0.21834649]\n",
      " [0.5618489 ]\n",
      " [0.5625683 ]\n",
      " [0.86558974]\n",
      " [0.9028729 ]\n",
      " [0.91356784]\n",
      " [0.42838776]\n",
      " [0.43515512]\n",
      " [0.41961077]\n",
      " [0.40813094]\n",
      " [0.3728614 ]\n",
      " [0.4526418 ]\n",
      " [0.4369698 ]\n",
      " [0.43590862]\n",
      " [0.41881   ]\n",
      " [0.4190864 ]\n",
      " [0.42361116]\n",
      " [0.46502405]\n",
      " [0.45810854]\n",
      " [0.4636416 ]\n",
      " [0.4770077 ]\n",
      " [0.45058724]\n",
      " [0.9260989 ]\n",
      " [0.9252761 ]\n",
      " [0.88893205]\n",
      " [0.8693356 ]\n",
      " [0.87145936]\n",
      " [0.9162797 ]\n",
      " [0.63555187]\n",
      " [0.6766628 ]\n",
      " [0.85448265]\n",
      " [0.9184104 ]\n",
      " [0.83043236]\n",
      " [0.91666794]\n",
      " [0.9162685 ]\n",
      " [0.86969924]\n",
      " [0.93176025]\n",
      " [0.92281383]\n",
      " [0.90546364]\n",
      " [0.88913935]\n",
      " [0.86320955]\n",
      " [0.8887945 ]\n",
      " [0.8957769 ]\n",
      " [0.6293927 ]\n",
      " [0.88097775]\n",
      " [0.71040285]\n",
      " [0.9340908 ]\n",
      " [0.9210264 ]\n",
      " [0.87808   ]\n",
      " [0.89696676]\n",
      " [0.86336017]\n",
      " [0.9164518 ]\n",
      " [0.3962961 ]\n",
      " [0.3962961 ]\n",
      " [0.418032  ]\n",
      " [0.38004464]\n",
      " [0.39411002]\n",
      " [0.4675765 ]\n",
      " [0.47968334]\n",
      " [0.70891017]\n",
      " [0.83299565]\n",
      " [0.7921615 ]\n",
      " [0.901402  ]\n",
      " [0.89851713]\n",
      " [0.8411592 ]\n",
      " [0.869056  ]\n",
      " [0.7929367 ]\n",
      " [0.8929586 ]\n",
      " [0.87441653]\n",
      " [0.69608307]\n",
      " [0.8050986 ]\n",
      " [0.80923903]\n",
      " [0.7017255 ]\n",
      " [0.9504448 ]\n",
      " [0.63408554]\n",
      " [0.77252054]\n",
      " [0.542008  ]\n",
      " [0.59554356]\n",
      " [0.5107711 ]\n",
      " [0.64849186]\n",
      " [0.42090207]\n",
      " [0.40102586]\n",
      " [0.37964222]\n",
      " [0.5713878 ]\n",
      " [0.47039232]\n",
      " [0.6052849 ]\n",
      " [0.50283617]\n",
      " [0.6420093 ]\n",
      " [0.93750733]\n",
      " [0.93545896]\n",
      " [0.94315803]\n",
      " [0.82675964]\n",
      " [0.9244972 ]\n",
      " [0.91560286]\n",
      " [0.9065452 ]\n",
      " [0.7780142 ]\n",
      " [0.7784945 ]\n",
      " [0.6606556 ]\n",
      " [0.9384895 ]\n",
      " [0.9341506 ]\n",
      " [0.9047541 ]\n",
      " [0.8790682 ]\n",
      " [0.27997833]\n",
      " [0.7043753 ]\n",
      " [0.8698232 ]\n",
      " [0.8361635 ]]\n"
     ]
    }
   ],
   "source": [
    "y_hat_test = model.predict([\n",
    "            #deep_features_test['ECG_features_C'],\n",
    "            deep_features_test['ECG_features_T'],\n",
    "            handcrafted_features_test['ECG_features'],\n",
    "            handcrafted_features_test['GSR_features']\n",
    "            ])\n",
    "print(y_hat_test)\n",
    "th = 0.5\n",
    "#y_hat_test[np.argwhere(np.isnan(y_hat_test))]= 0\n",
    "y_hat_test[y_hat_test >= th] = 1\n",
    "y_hat_test[y_hat_test < th] = 0\n",
    "np.savetxt(os.path.join('submission', 'answer.txt'),y_hat_test.astype(int),fmt='%1d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d275b6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d875e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e49260d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
